{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc593f0-a5bd-471b-9a14-a355758071fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# a simple self-attention mechanism calculation for a randomly taken encoded input sequence \n",
    "# values with a single head. The actual Transformer architecture used multi-headed attention\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a simple encoded sentence\n",
    "sentence = torch.tensor([0, 2, 8, 4, 1, 8, 5, 2])\n",
    "# embed sentence tensor with 10 vocab_size and 16 embed_dim\n",
    "torch.manual_seed(42)\n",
    "embed = torch.nn.Embedding(10, 16)\n",
    "# each element in sentence tensor is converted to 16-dim embedded tensor. So shape is (8X16)\n",
    "embedded_sentence = embed(sentence)\n",
    "\n",
    "# calcualte query, key and value role projection tensors for\n",
    "\n",
    "# step1: define query, key, value weights to calculate above q, k, v\n",
    "# shapes of these values are going to be dXd where shape of input sequence is nXd\n",
    "# the input seq has shape 8X16. so the weights have shape 16X16 because these weights are \n",
    "# to be dotted with each input item in the sequence to generate corresponding q, k and v values of each input item\n",
    "d = embedded_sentence.shape[1]\n",
    "torch.manual_seed(42)\n",
    "W_query = torch.rand(d, d)\n",
    "W_key = torch.rand(d, d)\n",
    "W_value = torch.rand(d, d)\n",
    "\n",
    "# calculate query, key and value projection matrices\n",
    "queries = torch.matmul(embedded_sentence, W_query)\n",
    "keys = torch.matmul(embedded_sentence, W_key)\n",
    "values = torch.matmul(embedded_sentence, W_value)\n",
    "\n",
    "# calculate alignment scores by dot product(.matmul() internally is bunch of dot products) \n",
    "# now we have the scores for how each input is related to or similar to other input items in the enitre sequence \n",
    "omega = torch.matmul(queries, keys.T)\n",
    "\n",
    "# normalise the values for prob distribution using softmax\n",
    "attn_weights = F.softmax(omega / d**0.5, dim=1)\n",
    "print(attn_weights.sum(dim=1))\n",
    "\n",
    "#weigted sum over 'values' to calcualte the output of self-attention layer\n",
    "# finally we represented the input sequence in a different way by calculating the self-attention\n",
    "z = torch.matmul(attn_weights, values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
